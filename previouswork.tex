\section{Previous Work}

There is a story that at some point, Göran Sundholm and Per Martin-Löf were
sitting at a dinner table, dicsussing various questions of interest to the
respective scholars, and Sundholm presented Martin-Löf with the problem of
Donkey Sentences in natural language semantics, those analogous 'Every man who
owns a donkey beats it'. This had been puzzling to those in the Montague
tradition, whereby higher order logic didn't provide facile ways of interpreting
these sentences. Martin-Löf apparently then, using his dependent type
constructors, provided an interpretation of the donkey sentence on the back of
the napkin. This is perhaps the genesis of dependent type theory in natural
language semantics. The research program was thereafter taken up by Martin-Löf's
student Aarne Ranta \cite{ranta1994type}, bled into the development of GF, and
has now in some sense led to this current work.

The prior exploration of these interleaving subjects is vast, and we can only
sample the available literature here. Indeed, there are so many approaches that
this work should be seen in a small (but important) case in the context of a
deep and broad literature \cite{surveyLang}. Acquiring expertise in such a
breadth of work is outside the scope of this thesis. Our approach, using
GF ASTs as a basis language for Mathematics and the logic the mathematical
objects are described in, is both distinct but has many roots and
interconnections with the remaining literature. The success of finding a
suitable language for mathematics will obviously require a comparative analysis
of the strengths and weaknesses in the goals in such a vast bibliography. 
 How the GF approach compares with this long merits careful consideration and
 future work.

It will function of our purpose, constrained by the limited scope of this work,
to focus on a few important resources.

\subsection{Ranta}

The initial considerations of Ranta were both oriented towards the language of
mathematics \cite{ranta93}, as well as purely linguistic concerns
\cite{ranta1994type}. In the treatise, Ranta explored not just the many avenues
to describe NL semantic phenomena with Dependent Types, but, after concentrating
on a linguistic analysis, he also proposed a primitive way of parsing and
sugaring these dependently typed interpretations of utterances into the strings
themselves - introducing the common nouns as types idea which has been since
seen great interest from both type theoretic and linguistic communities
\cite{luoCNs}. Therefore, if we interpret the set of men and the set of donkeys
as types, e.g. we judge $\vdash man \; {:} {\rm type}$ and $\vdash donkey \; {:}
{\rm type}$ where ${\rm type}$ really denotes a universe, and ditransitive verbs
``owns'' and ``beats'' as predicates, or dependent types over the CN types, i.e.
$\vdash owns \; {:} man \rightarrow donkey \rightarrow {\rm type}$ we can
interpret the sentence ``every man who owns a donkey beats it'' in DTT via the
following judgment :

\[\Pi z : (\Sigma x : man. \; \Sigma y : donkey. \; owns(x,y)). \; beats(\pi_1z,\pi_1(\pi_2z))\]

We note that the natural language quantifiers, which were largely the subject of
Montague's original investigations \cite{Montague1973}, find a natural
interpretation as the dependent product and sum types, $\Pi$ and $\Sigma$,
respectively. As type theory is constructive, and requires explicit witnesses
for claims, we admit the behavior following semantic interpretation : given a
man $m$, a donkey $d$ and evidence $m-owns-d$ that the man owns the donkey, we
can supply, via the term of the above type applied to our own tripple
$(m,d,m-owns-d)$ , evidence that the man beats the donkey, $beats(m,d)$ via
$pi_1$ and $pi_2$, the projections, or $\Sigma$ eliminators.

In the final chapter of \cite{ranta1994type}, $Sugaring and Parsing$, Ranta
explores the explicit relation, and of translation between the above logical
form and the string, where he presents a GF predecessor in the Alfa proof
assistant, itself a predecessor of Agda. To accomplish this translation he
introduces an intermediary , a functional phrase structure tree, which later
becomes the basis for GFs abstract syntax.  What is referred to as ``sugaring''
later changes to ``linearization''.

Soon thereafter, GF became a fully realized vision, with better and more
expressive parsing algorithms \cite{ljunglof2004expressivity} developed in
Göteborg allowed for sugaring that can largely accommodate morphological
features of the target natural language \cite{rantaForsberg}, the translation
between the functional phrase structure (ASTs) and strings \cite{ranta_2004}.

Interestingly, the functions that were called $ambiguation : MLTT \to \{Phrase\
Structure\}$ and $interpretation : \{Phrase Structure\} \to MLTT$ were absorbed
into GF by providing dependently typed ASTs, which allows GF not just to parse
syntactic strings, but only parse semantically well formed, or meaningful
strings. Although this feature was in some sense the genesis that allowed GF to
implement the lingusitic ideas from the book \cite{rantaTT}, it has remained
relatively limited in terms of actual GF programmers using it in their day to
day work. Nonetheless, it was intriguing enough to investigate briefly during
the course of this work as one can implement a programming language grammar that
only accepts well typed programs, at least as far as they can be encoded via
GF's dependent types \cite{warrickHarper}. Although GF isn't designed with
TypeChecking in mind explicity, it would be very interesting to apply GF
dependent types in the more advanced programming languages to filter parses of
meaningless strings.

While the semantics of natural language in MLTT is relevant historically, it is
not the focus of this thesis. Its relevance comes from the fact that all these
ideas were circulating in the same circles - that is, Ranta's writings on the
language of mathematics, his approach to NL semantics, and their confluence
among other things, with the development of GF. This led to the development of a
natural language layer to Alfa \cite{alfaGF}, which in some sense can be seen as
a direct predecessor to this work. In some sense, the scope of work seeks to
recapitulate what was already done in 1998 - but this was prior to both GF's
completion, and Alfa's hard fork to Agda.

\subsubsection{Prior GF Formalizations}

Prior to the grammars explored thin this thesis, Ranta produced two main results
\cite{rantaLog} \cite{aarneHott}. These are incredibly important precedents in
this approach to proof translation, and serve as important comparative work for
which this work responds. In \cite{rantaLog}, Ranta designed a grammar which
allowed for predicate logic with domain specific lexicon supporting mathematical
theories on top of the logic like geometry or arithmetic. The the syntax was
both meant to be relatively complete, so that typical logical utterances of
interest could be accommodated, as well as relatively non-trivial linguistic
nuance including lists of terms, predicates, and propositions, in-situ and
bounded quantification, and multiple forms of constructing more syntactically
nuanced predicates. The syntactic nuance captured in this work was by means of an
extended grammar, via a Portable Grammar Format (PGF), on top of the minimal,
core logical formalism.

One could translate from the core and extended via a denotational semantics
approach. The tree representing the \emph{syntactically complete} phrase ``for
all natural numbers x, x is even or x is odd" would be evaluated to a tree which
linearizes to the \emph{semantically adequate} phrase ``every natural number is
even or odd". In the opposite direction, the desugaring of a logically informal
statement into something linguistically idiomatic is also accomplished. In some
sense, this grammar serves as a case study for what this thesis is trying to do.
However, we note that the core logic only supports propositions without proofs -
it is not a type theory with terms. This means that we are being slightly
abusive to our terms, as the formal/informal translation is taking place is at
the PGF level. The GF translation between concrete syntaxes supports multiple
NLs, but the syntactic completeness has no mechanism of verification via Agda's
type checker. Additionally, the domain of arithmetic is an important case study,
but scaling this grammar (or any other, for that matter) to allow for
\emph{semantic adequacy} of real mathematics is still far away, or as Ranta
concedes, ``it seems that text generation involves undecidable optimization
problems that have no ultimate automatic solution." It would be interesting to
further extend this grammar with both terms and an Agda-like concrete syntax. 

In 2014, Ranta gave an unpublished talk at the Stockholm Mathematics seminar
\cite{aarneHott}. Fortunately the code is available, although many of the design
choices aren't documented in the grammar. This project aimed to provide a
translation like the one desired in our current work, but it took a real piece
of mathematics text as the main influence on the design of the Abstract syntax.

This work took a page of text from Peter Aczel's book which more or less goes
over standard HoTT definitions and theorems, and allows the translation of the
latex to a pidgin logical language. The central motivation of this grammar was
to capture, entirely ``real" natural language mathematics, i.e. that which was
written for the mathematician. Therefore, it isn't reminiscent of the slender
abstract syntax the type theorist adores, and sacrificed ``syntactic
completeness" for ``semantic adequacy". This means that the abstract syntax is
much larger and very expressive, but it no longer becomes easy to reason about
and additionally quite ad-hoc. Another defect is that this grammar
overgenerates, so producing a unique parse from the PL side will become tricky.
Nonetheless, this means that it's presumably possible to carve a subset of the
GF HoTT abstract file to accommodate an Agda program, but one encounters rocks as soon
as one begins to dig. For example, in \autoref{fig:M1} is some rendered latex
taken verbatim from Ranta's test code.

With some of hours of tinkering on the pidgin logic concrete syntax and some
reverse engineering with help from the GF shell, one is able to get these
definitions in \autoref{fig:M2}, which are intended to share the same syntactic
space as cubicalTT. We note the first definition of ``contractability" actually
runs in cubicalTT up to renaming a lexical items, and it is clear that the
translation from that to Agda should be a benign task. However, the
\emph{equivalence} syntax is stuck with the artifact from the bloated abstract
syntax for the of the anaphoric use of ``it", which may presumably be fixed with
a few hours more of tinkering, but becomes even more complicated when not just
defining new types, but actually writing real mathematical proofs, or relatively
large terms. To extend this grammar to accommodate a chapter worth of material,
let alone a book, will not just require extending the lexicon, but encountering
other syntactic phenomena that will further be difficult to compress when
defining Agda's concrete syntax.

\begin{figure}

 \textbf{Definition}:
 A type $A$ is contractible, if there is $a : A$, called the center of contraction, such that for all $x : A$, $\equalH {a}{x}$.

 \textbf{Definition}:
 A map $f : \arrowH {A}{B}$ is an equivalence, if for all $y : B$, its fiber, $\comprehensionH {x}{A}{\equalH {\appH {f}{x}}{y}}$, is contractible.
 We write $\equivalenceH {A}{B}$, if there is an equivalence $\arrowH {A}{B}$.
\caption{Rendered Latex} \label{fig:M1}


\begin{verbatim}
isContr ( A : Set ) : Set = ( a : A ) ( * ) ( ( x : A ) -> Id ( a ) ( x ) )

Equivalence ( f : A -> B ) : Set = 
  ( y : B ) -> ( isContr ( fiber it ) ) ; ; ; 
  fiber it : Set = ( x : A ) ( * ) ( Id ( f ( x ) ) ( y ) )
\end{verbatim}
\caption{Pidgin cubicalTT} \label{fig:M2}

\input{latex/ContrClean}
% \input{latex/ex}
% \input{latex/primitives}

\caption{Agda} \label{fig:M3}
\end{figure}

Additionally, we give the Agda code in \autoref{fig:M3}, so-as to see what the
end result of such a program would be. The astute reader will also notice a
semantic in the pidgin rendering error relative to the Agda implementation.
\codeword{fiber} has the type \codeword{it : Set} instead of something like
\codeword{(y : B) : Set}, and the y variable is unbound in the \codeword{fiber}
expression. This demonstrates that to design a grammar prioritizing
\emph{semantic adequacy} and subsequently trying to incorporate \emph{syntactic
completeness} becomes a very difficult problem. Depending on the application of
the grammar, the emphasis on this axis is most assuredly a choice one should
consider up front.

While both these grammars have their strengths and weaknesses, one shall see
shortly that the approach in this thesis, taking an actual programming language
parser in Backus-Naur Form Converter (BNFC), GFifying it, and trying to use the
abstract syntax to model natural language, gives in some sense a dual challenge,
where the abstract syntax remains simple, but its linearizations become
must increase in complexity.

% Draw a figure with these as axes, the names of the grammars, and a z axis for
% size of lexicon, and basically how can we optimize all three

\subsection{Mohan Ganesalingam}


\begin{displayquote}

there is a considerable gap between what mathematicians claim is true and what
they believe, and this mismatch causes a number of serious linguistic problems

\end{displayquote}

Perhaps the most substantial analysis of the linguistic perspective on
written mathematics comes from Ganesalingam \cite{ganesalingam2013language}.
Not only does he pick up and reexamine much of Ranta's early work, but he
develops a whole theory for how to understand with the language mathematics from
a formal point of view, additionally working with many questions about the
foundation of mathematics. His model which is developed early in the treatise
and is referenced 
throughout uses Discourse Representation Theory \cite{kamp2011discourse}, to
capture anaphoric use of variables. While he is interested in analyzing
language, or goal is to translate, because the meaning of an expression is
contained in its set of formalizations, so GF is more of just a tool in the
pipeline rather than an actual infrastructure through which to dissect the various
nuances of human speech.


"
meaningful statements in some underlying logic. If it was pointed out that
a particular sentence had no translation into such a logic, a mathematician
would genuinely feel that they had been insufficiently precise. (The actual
translation into logic is never performed, because it is exceptionally laborious;
"

"
mathematics has a normative notion of what its content should look like; there is no
analogue in natural languages.
"

1.2.3 full adaptivity


"
From a linguistic perspective, the formal mode is more novel and interesting
because it is restricted enough to describe completely, both in terms of syntax
and semantics. By contrast, the informal mode seems as hard to describe as
general natural language. We will therefore look only at mathematics in the
formal mode.
"


Section 2

"
The primary function of symbolic mathematics is to abbreviate material
that would be too cumbersome to state with text alone. Thus a sentence
"

"
Because symbolic material functions primarily in an abbreviative capacity,
symbolic mathematics tends to occur inside textual mathematics rather than
vice versa. Thus mathematical texts are largely composed out of textual
"

"
adaptivity
a phenomenon that is much more remarkable than the use of symbols. Math-
ematical language expands as more mathematics is encountered. The kind of
"

"
Thus definitions always contain enough information to fully specify the
semantics of the material being defined.
"

"
As a result, textual mathematics predominantly
uses the third person singular and third person plural, to denote individual
"

"
verbs, typically to refer to the mutual intent of the author and reader.
Working mathematicians treat mathematical objects as if they were Pla-
tonic ideals, timeless objects existing independently of the physical world. The
"

"
The limited variation in person and tense means that inflectional morphol-
gy plays only a small part in mathematical language. The only morphological
"

"The syntax of textual mathematics also exhibits relatively limited variation."


this means that textual mathematics can be effectively captured by a context-
free grammar (in the sense of (Jurafsky and Martin, 2009, p. 387)).

In contrast to the morphology and syntax of textual mathematics, its
lexicon is remarkably varied. As we have noted above, the mechanism of

no tense or events
no intesnionality
no modality

"
usual. To a first approximation, mathematics does not exhibit any pragmatic
phenomena: the meaning of a mathematical sentence is merely its composition-
ally determined semantic content, and nothing more. In order to state this point
"


"
Due to the absence of pragmatic phenomena, phenomena which are some-
times analysed as semantic and sometimes analysed as pragmatic can be
treated as being purely semantic where they occur in mathematics, i.e. they
can be analysed in purely truth-conditional terms. This applies particularly
to presuppositions, which play an important role in mathematics. Because
"

"
Thus, in some intuitive sense, syntax is dependent on the types of expres-
sions in a way that does not occur in existing formal languages. As we will
show in §3.2 and Chapter 4, this notion of type is too semantic itself to be
formalised in syntactic terms. In other words, the type of an expression is too
closely related to what that expression refers to for purely syntactic notions
of type to be applicable.
"

"
most ambiguity in mathematics is not noticed by mathematicians, just as the extensive ambi-
guity in natural languages is “simply not noticed by the majority of language
"

"
in an extremely compact manner. In essence, they serve as a mathematical
alternative to anaphor. They cannot be eliminated precisely because anaphor
"

reanalysis

Chapter 7 pg 181

be equal as distinct. In both cases, a disparity between the way we think
about mathematical objects and the way they are formally defined causes
our linguistic theories to make incorrect predictions. In order to obtain the
correct predictions about language, we need to make sure that the formal
situation matches actual usage.

mal proofs are provided; and the cycle repeats. Thus informal mathematics
changes over the centuries. 187

% my idea
engineers are motivated by needs and desires (emirical, descriptive,
practial) , whereas mathematicians are much more idealistic in their pursuits,
almost comparable to a stances on religious scripture

mathematics developing over time is natural, the more and deeper we dig into the
ground, the more we develop refinements of what kind of tools we are using,
develop better iterations of the same tools (or possibly entirely new ones) as
well as knowledge about the ground in which we are digging (these are adjoining)

in some sense the library of babel problem, whereby we dont just discover
predefined ideas by randomly sampling bags of words, but we have to work with
hard labor, sweat, and tears, to imbue the sentences of mathematics with meaning
that makes them descriptive, that there is some kind of internal, but
distributed, mental process which is mirror whats on paper (and the 'reality' it
describes)

relate this to HoTT as a perfect 'case study' in the foundations of mathematics


\subsection{other authors}

The question 

We note that 

NaProche

he Naproche project (Natural language Proof Checking) studies the semi-formal language of mathematics from a linguistic, philosophical and mathematical perspective. A central goal of Naproche is to develop a controlled natural language (CNL) for mathematical texts and adapted proof checking software which checks texts written in the CNL for syntactical and mathematical correctness. 

Uses Automated Theorem Prover (ATP) backend rather than ITP


Mizar

Coq (coquand), 

Boxer
Boxer system [29] is able to parse any English sentence and translate it into a formula
of predicate calculus. Combined with theorem proving and model checking, Boxer can
moreover solve problems in open-domain textual entailment by formal reasoning [30].
The problem that remains with Boxer is that the quality of formalization and rea-
