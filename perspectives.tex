\section{Perspectives}

\begin{displayquote}

...when it comes to understanding the power of mathematical language to guide our
thought and help us reason well, formal mathematical languages like the ones
used by interactive proof assistants provide informative models of informal
mathematical language. The formal languages un- derlying foundational frameworks
such as set theory and type theory were designed to provide an account of the
correct rules of mathematical reasoning, and, as Gödel observed, they do a
remarkably good job. But correctness isn’t everything: we want our mathematical
languages to enable us to reason efficiently and effectively as well. To that
end, we need not just accounts as to what makes a mathematical argument correct,
but also accounts of the structural features of our theorizing that help us
manage mathematical complexity.\cite{avigad2015mathematics}

\end{displayquote}

The key development of this thesis it to explore the formal/informal distinction
in mathematical language via by means of rule-based, syntax oriented machine
translation.

Computational linguistics, particularly those in the tradition of type
theoretical semantics\cite{ranta1994type}, gives one a way of comparing natural
and programming languages. Although it is concerned with the semantics of
natural language in the logical tradition of Montague, which who synthesized
work in the traditions of Chomsky \cite{Chomsky57} and Frege \cite{frege79}, it
ended up inspiring the GF system, a side effect of which was to realize that
translation was possible with this of abstracted view of natural language
semantics. Indeed, one such description of GF is that it is a compiler tool
applied to domain specific machine translation. One can then carry this analogy
to other areas of computer science, and compare NL and PL phenomena generally.

We will refer to this as the programming language and linguistic abstraction
ladders, and after viewing \autoref{fig:M1}, the reader should try to comprehend
this comparison with her own knowledge and expertise in mind. These respective
ladders are perhaps the most important side-by-side comparison one should keep
in mind while reading this thesis. Importantly, we should observe that the PL
dimension, the left diagram, represents synthetic processes, those which we
design, make decisions about, and describe formally. Alternatively, the NL
abstractions on the right represent analytic observations, and therefore are
subject to different, in some ways orthogonal, constraints.

The linguistic abstractions are subject to empirical observations and
constraints, and it is therefore this diagram only serves as an atlas for the
different abstractions and relations between these abstractions, which may be
subject to modifications depending on the linguist or philosopher investigating
such matters. The PL abstractions as represented, while also an approximations,
serves as an actual high altitude blueprint for the design of programming
languages, and while the devil is in the details and this view is greatly
simplified, the representation of PL design is unlikely to create angst in the
computer science community. The linguistic abstractions, in some sense, are at
the intersection of many fascinating debates between all many different
communities, and there is certainly nothing close to any type of consensus among
them which linguistic abstractions, as well as their arrangement one wishes to
pursue.

\begin{figure}
\centering
\begin{tikzcd}
Strings \ar[d,"Lexical\ Analysis"] \ar[dd,bend right=+90.0, swap,"Front\ End"]
&[5m]
\\ Lexemes \ar[d,"Parsing"] &[5em]
\\ ASTs \ar[d,"Type\ Checker"] &[5m]
\\ Typed\ ASTs
  \ar[dd, bend left, "Code\ Generator"] 
  \ar[dd, bend right, swap, "Interpreter"] &[5m]
\\ ...
\\ Target\ Language
\end{tikzcd}
\hspace{1cm}
\begin{tikzcd}
  Phonemes \arrow[d, "Morhphophonological
  \\ Anaylsis" description]
  \\ Morphemes \arrow[d, "Parse"]
  \\ \{\ Syntactic\ Representation\ \} \arrow[d, "Montague"', bend right=49]
    \arrow[d, "Ranta", bend left=49] \arrow[d, "..." description]
  \\ {\{\ STLC,\ ...\ ,\ DTT\ \}} \arrow[d, "?" description]
  \\ {\{\ Nearal Encoding\ ,\ ...\ Internal\ Language\ \}} \arrow[d, "?" description]
  \\ Phonemes
\end{tikzcd}
\caption{Abstraction Ladders} \label{fig:M1}
\end{figure}


There are also many relevant concerns not addressed in either abstraction chain
that are necessary to give a more comprehsive snapshot. For instance, on the PL
side, we can inquire about 

extrinsic categories

(i) systems with multiple interactive programming language 
(ii) how the programming languages behave with respect to given programs
(iii) embedding programming languages

intrinsic, how more advanced programming, like Agda which requires the
evaluation of terms during typechecking, for instance, is
implemented with 4.5 different stages between the syntax written by the
programmers and the Reflected AST.  But this is perhaps an unfair
characterization, because Agda's typechecker is so powerful that the design,
implemenation, and use of Agda revolves around its type-checker, (which,
ironically, is already called during the parsing phase). It is not
anticipated that floating point computation, for instance, would ever be
considered when implementing new features of Agda. Indeed, the ways Agda
represents ASTs were an obstacle encountered doing this work, because deciding
which stage one should connect with via the PGF embedding is nontrivial, and
isolating these intermediary AST stages, which are meant to facilitate as
interlingua when evaluating an Agda program.

In the linguistic case,

We want to highlight that high
altiude abstraction 


On the other hand, some
languages are only implemented 


with different amounts of
desugaring occurring during the stages. Agda's parser, 


% 0. unicode: before parsing
% 1. Concrete: after happy parsing Parser/Parser.y that does a little desugaring already---[but ideally shouldn't].  Expressions are not parsed yet.
% 1.5 Concrete.Definitions: the "nice" syntax after the nicifier, preparing for the scope checker
% 2. Abstract: after scope checking, mostly desugared, expressions parsed
% 3. Internal: after type checking, fully desugared
% (4. Reflected: quoted from Internal)

from  


Let's zoom in a little and observe the so-called front-end part of the compiler. 

\begin{tikzcd}
Strings \ar[r,"Lexical Analysis"] \ar[rr,bend right,"GF Parser"'] &[10em] Lexemes
\ar[r,"Parsing"] &[10em] ASTs \ar[ll,bend right, "GF Linearization"] 
\end{tikzcd}

This also has some (external) mapping at the neurological level, where one
somehow can say the internal language, mechanism of externalization (generally
speech), but we won't be concerned with that here. The point is to recognize
their are stark differences, and that classifying both programming languages and
natural languages as languages is best read as an incomplete (and even sometimes
contradictory) metaphor, due to perceived similarities (of which their are
ample).

Nonetheless, the point of this thesis is to take a crack at that exact question
: how can one compare programming and natural languages, in the sense that a
natural language, when restricted to a small enough (and presumably
well-behaved) domain, behaves as a programming language. And simultaneously, we
probe the topic of Natural Language Generation (NLG), in the sense that given a
logic or type system with some theory inside (say arithmetic over the naturals),
how do we not just find a natural language representation which interprets our
expressions, but also does so in a way that is linguistically coherent in a
sense that a competent speaker can read such an expression and make sense of it.

The specific linguistic domain we focus on, that of mathematics, is a particular
sweet spot in the intersection of many intersecting interests. It should be
noted that this problem, that of translating between formal (in a PL or logic)
and informal (in linguistic sense) mathematics as stated, is both vague and
difficult. It is difficult in both the computer science sense, that it may be
either of infeasible complexity or even perhaps undecidable. But it is also
difficult in the philosophical sense, that it a question which one may come up
with a priori arguements against its either effectiveness or meaningfulness.
Because, like all collective human endeavors, mathematics is a historical
construction - that is, its conventions, notations, understanding,
methodologies, and means of publication and distribution have all been in a
constant flux. While in some sense the mathematics today can be seen today as a
much refined version of whatever the Greeks or Egyptians were doing, there is no
consensus on what mathematics is, how it is to be done, and most relevant for
this treatise, how it is to be expressed.

We present a sketch of the difference of this so-called formal/informal
distinction. Mathematics, that is mathematical constructions like numbers and
geometrical understandings, arose out of ad-hoc needs as humans cultures grew
and evolved over the millennia. Indeed, just like many of the most interesting
human developments of which there is a sparsely documented record until
relatively recently, it is likely to remain a mystery what the long historical
arc of mathematics could have looked like in the context of human evolution. And
while mathematical notions precede mathematical constructions (the spherical
planet precedes the human use of a ruler compass construction to generate a
circle), we should take it as a starting point that mathematics arises naturally
out of our linguistic capacity. This may very well not be the case, or at least,
not the case in all circumstances. But it is impossible for me to imagine a
mathematical construction elaborating anything particularly general without
linguistic faculties. This contention may find both empirical or philosophical
dispute, but the point is to make a first order approximation for the sake of
this presentation and perspective. The debate of mathematics and its relation to
linguistics generally, regardless of the stance one takes, should hopefully
benefit from the work presented in this thesis regardless of ones stance.

  * syntactic Completeness and semantic adequacy

The GF pipeline, that of bidirectional translation through an intermediary
abstract syntax representation, has, two fundamental criteria that must be
assessed for one to judge the success of the approach for the informal/formal
translation. The first, which we'll call *syntactic completeness*, asks the
following : given an utterance or natural language expression that a
mathematician might understand, does the GF grammar emit a well-formed syntactic
expression in the target logic or programming language? [Example?]

This problem is certain to be infeasible in many cases - a mathematician might
not be able to reconstruct the unstated syntactic details of a proof in an
discipline outside her expertise, it is at worthy pursuit to ask why it is so
difficult! Additionally, one does not know a priori that the generated
expression in the logic has its intended meaning, other than through some meta
device (like for instance, some meaning explanation outside verification
procedure).

Conversely, given a well formed syntactic expression in, for instance, Agda, one
can ask if the resulting English expression generated by GF is *semantically
adequate*. This notion of semantic adequacy is also delicate, as mathematicians
themselves may dispute, for instance, the proof of a given proposition or the
correct definition of some notion. However, if it is doubtful that there would
be many mathematicians who would not understand some standard theorem/proof pair
in a 7th edition introductory real analysis text, even if they dispute it's
presentation, clarity, pedagogy, or other pedantic details. So, whether one asks
that semantic adequacy means some kind of sociological consensus among those
with relevant expertise, or a more relaxed criterion that some expert herself
understands the argument (a possibly dubious perspective in science), semantic
adequacy should appease at least one and potentially more mathematicians.

We introduce these terms, syntactic completeness and semantic adequacy, to
highlight a perspective and insight that seems to underlie the biggest
differences between informal and formal mathematics. We claim that mathematics,
as done on a theorem prover, is a syntax oriented endeavor, whereas mathematics,
as practiced by mathematicians, prioritizes semantic understanding.

This perspective represents an observation, and not intended to take a side as
to whether the syntactic or semantic perspective on mathematics is better -
there is an dialectical phenomena between the two.

Let's highlight some advantages both provide, and try to distinguish more
precisely what a syntactic and semantic perspective may be.

When the Agda user builds her proof, she is outsourcing much of the bookkeeping
to the type-checker. This isn't purely a mechanical process though, she often
does have to think, how her definitions will interact with typing and term
judgments downstream, as well as whether they are even sensible to begin with
(i.e. does this have a proof). The syntactic side is expressly clear from the
readers perspective as well. If Agda proofs were semantically coherent, one
would only need to look at code, with perhaps a few occasional remarks about
various intentions and conclusions, to understand the mathematics being
expressed. Yet, papers are often written exclusively in Latex, where Agda proofs
have had to be reverse engineered, preserving only semantic details and
forsaking syntactic nuance, and oftentimes the code is kept in the appendix so
as to provide a complete syntactic blueprint. But the act of writing an Agda
proof and reading them are often orthogonal, as the term somehow shadows the
application of typing rules which enable its construction. In some sense, the
construction of the proof is entirely engaged with the types, whereas the human
witness of a term is either lost as to why it fulfills the typing judgment, or
they have to reconstruct the proof in their head (or perhaps, again, with Agda's
help).

Even in cases where Agda code is included in the paper, it is often the types
that emphasized (and read), and complex proof terms are seldom to be read on
their own terms. The natural language description and commentary is still
largely necessary to convey whatever results, regardless if the Agda code is
self-contained. And while literate Agda is some type of bridge, it is still the
commentary, which in some sense unfolds the code, which makes the Agda legible.

This is particularly pronounced in Coq, where proof terms are built using LTac,
which can be seen as some kind of imperative syntactic metaprogramming over the
core language, Gallina. The Tactics themselves are not typed, often feel very
adhoc, and carry very little semantic value (or even possibly muddy one's
understanding when reading proofs with unknown tactics). Indeed, since LTac
isn't itself typed, it often descends into the sorrows of so-called untyped
languages (which really are really unityped), and there are multiple arguements
that this should be changed. But from our perspective, the use of tactics is an
additional syntactic obfuscation of what a proof should look like from the
mathematicians perspective - and attempt to remedy this is. This isn't always
the case, however, as tactics like `ring` or `omega` often save the reader
overhead of parsing pendantic and uninforamtive details. And for certain proofs,
especially those involving many cases, the metaprogramming facilities actually
give one exclusive advantages not offered to the classical mathematician using
pen and paper. Nonetheless, the dependent type theorist's dream that all
mathematicians begin using theorem provers in their everyday work is largely
just a dream, and with relatively little mainstream adoption by mathematicians,
the future is all but clear.

Mathematicians may indeed like some of the facilities theorem provers provide,
but ultimately, they may not see that as the "essence" of what they are doing.

----------------------------------------------------------------

What is a proof? 

Let
Human 1 := h1
Human 2 := h2
. If h1 claims to have a proof p1, and elaborates it to h2 who claims she can
either verify p1 or 
reproduce and re-articulate it via p1', such that h1 and h2 agree that p1 and
p1' are equivalent, then we have discovered some mathematics.agree Consensus of h1


  Formalization = formal model construction of an informal piece of text
  model could be different for different formalizations

  - it may be formalized differently by two different people in many different ways
  - it may have to be modified, to correct of an error, or modify a smaller piece?
  - it may not type check, and only be presumed hypothetically to be 'a correct formalization'


  
What is a proof? It is a series of lemmas, constructors, and destructors

What is informalization? 

It is a process of taking a formal syntax, and turning it into a natural
language utterance -  It is a clarification of the meaning of a piece of
code. suppressing certain details and reiterating, sometimes
redundantly, other details.

A metaphor of the architect and the engineer.  A building, like all human
endeavors, is created via resources and labor of many people. The role of the archtitect
is to envision the gilded facade, the exterior layer directly perceived by
others, giving a building its character, its purpose and function hidden in
either specific articulations like the
smokestack,  its 

Dialectal layers of mathematics.  From the formal to the expository 

Chicken or the egg?

In order to construct a model of mathematics, we must a priori have a
mathematical out of which to construct it.  Meaning Explanations?

What is a foundation of mathematics from a mathematical perspective vs from a
philosophical perspective

That in some sense, mathematics and mathematicians seek model independence in
their results (i.e., they don't need a direct encoding of Fermat's last theorem
in set theory in order to trust its validity). This is one possible reason why
there is so much reluctance to adopt proof assistant, that Agda 2.6.*, and its
standard library, may break future   - and  we believe the GF approach offers at
least a vision of not only linguistic, but also foundation independence with
respect to mathematics.


there is a considerable gap between what math-
ematicians claim is true and what they believe, and this mismatch causes a
number of serious linguistic problems
\cite{ganesalingam2013language}

--for equality section
Sense vs reference, in some sense mathematics should be concerned with the
reference of mathematical claims, independent of the language they are being
stated in.  The abstractions of mathematics, while compositionally built out of
the piece (which, classically, end up as sets), behave like their own things,
with emergent properties (What Buckminster Fuller describes as syngergy)
so that in the same way that a helium molecule's behavior is not theoretically
tractable (numerical approximations are necessary) much of mathematics is similar

also think, analytic vs synthetic distinction

this gives us a new perspective on HoTT, because the description of spaces and
other topological constructions, via HITs, uses a more sophisticated
foundational machinery to connect the semantic intuition of a space with its
syntactic description (rather than defining Homotopies classically over R or C, etc)


-- this is a current phenomena

"
meaningful statements in some underlying logic. If it was pointed out that
a particular sentence had no translation into such a logic, a mathematician
would genuinely feel that they had been insufficiently precise. (The actual
translation into logic is never performed, because it is exceptionally laborious;
"


"
mathematics has a normative notion of what its content should look like; there is no
analogue in natural languages.
"

1.2.3 full adaptivity


From a linguistic perspective, the formal mode is more novel and interesting
because it is restricted enough to describe completely, both in terms of syntax
and semantics. By contrast, the informal mode seems as hard to describe as
general natural language. We will therefore look only at mathematics in the
formal mode.


% --
% Philosophical Links

This thesis takes not just a practical problem, but touches many deep issues in
some space in the intersection of the foundations, both practical and
philosophical, of mathematics, logic, computer science, and their relations
studied via linguistic formalisms. These subjects, and their various relations,
are the subject of countless hours of work and consideration by many great
minds. This thesis barely scratches the surface of a few of these developments,
but it nonetheless, it is hoped, provides a nontrivial perspective at many
important issues.

For instance, what are mathematical objects, how do their encodings in different
foundational formalisms effect their interpretations, how does is mathematics
develop as a social process, and how does what mathematics is and how it is done
rely on given technologies of a given historical era. And while various branches
of linguistics has
seen rapid evolution due to, in large part, its adoption of mathematical tools,
the dual application of linguistic tools to mathematics is quite sparse and open
terrain.  We hope the reader can walk away with an new appreciation to some of
these questions and topics after reading this thesis.

These nuances we will not explore here, but do intend to further elaborate in the
future and and more importantly, inspire other readers to respond accordingly.

Although not given in specific detail, the view of what mathematics is, in both
a philosophical and mathematical sense, as well as from the view of what a
foundational perspective, requires deep consideration in its relation to
linguistics. And while this work is perhaps just a finer grain of sandpaper on
an incomplete and primordial marble sculpture, it is hoped that the sculptors
own reflection is a little bit more clear after we polish it here.


