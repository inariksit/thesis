\section{Perspectives}


Computational linguistics, particularly those in the tradition of type
theoretical semantics (Ranta 94), gives one a way of comparing natural and
programming languages. Although it is concerned with the semantics of natural
language in the logical tradition of Montague, it ended up inspiring the GF
system, a side effect of which was to realize that translation was possible with
this of abstracted view of natural language semantics. One can then carry this
analogy to other areas of computer science, and compare NL and PL phenomena.
Indeed, one such description of GF is that it is a compiler tool applied to
domain specific machine translation.

* In PL theory, we also have a parallel series of abstraction levels, of which a
compiler writing course covers the basics. Lexemes -> Abstract Syntax Trees ->
Well Formed ASTs (i.e. trees which type checking) -> (possibly) intermediate
language Programs -> Target Language * In what I'll call formal linguistics, we
have some kind of hierarchy in terms of the following Phonemes -> Morhemes ->
Syntactic Units -> Sentences -> Dialogue

This also has some (external) mapping at the neurological level, where one
somehow can say the internal language, mechanism of externalization (generally
speech), but we won't be concerned with that here. The point is to recognize
their are stark differences, and that classifying both programming languages and
natural languages as languages is best read as an incomplete (and even sometimes
contradictory) metaphor, due to perceived similarities (of which their are
ample).

Nonetheless, the point of this thesis is to take a crack at that exact question
: how can one compare programming and natural languages, in the sense that a
natural language, when restricted to a small enough (and presumably
well-behaved) domain, behaves as a programming language. And simultaneously, we
probe the topic of Natural Language Generation (NLG), in the sense that given a
logic or type system with some theory inside (say arithmetic over the naturals),
how do we not just find a natural language representation which interprets our
expressions, but also does so in a way that is linguistically coherent in a
sense that a competent speaker can read such an expression and make sense of it.

The specific linguistic domain we focus on, that of mathematics, is a particular
sweet spot in the intersection of many intersecting interests. It should be
noted that this problem, that of translating between formal (in a PL or logic)
and informal (in linguistic sense) mathematics as stated, is both vague and
difficult. It is difficult in both the computer science sense, that it may be
either of infeasible complexity or even perhaps undecidable. But it is also
difficult in the philosophical sense, that it a question which one may come up
with a priori arguements against its either effectiveness or meaningfulness.
Because, like all collective human endeavors, mathematics is a historical
construction - that is, its conventions, notations, understanding,
methodologies, and means of publication and distribution have all been in a
constant flux. While in some sense the mathematics today can be seen today as a
much refined version of whatever the Greeks or Egyptians were doing, there is no
consensus on what mathematics is, how it is to be done, and most relevant for
this treatise, how it is to be expressed.

We present a sketch of the difference of this so-called formal/informal
distinction. Mathematics, that is mathematical constructions like numbers and
geometrical understandings, arose out of ad-hoc needs as humans cultures grew
and evolved over the millennia. Indeed, just like many of the most interesting
human developments of which there is a sparsely documented record until
relatively recently, it is likely to remain a mystery what the long historical
arc of mathematics could have looked like in the context of human evolution. And
while mathematical notions precede mathematical constructions (the spherical
planet precedes the human use of a ruler compass construction to generate a
circle), we should take it as a starting point that mathematics arises naturally
out of our linguistic capacity. This may very well not be the case, or at least,
not the case in all circumstances. But it is impossible for me to imagine a
mathematical construction elaborating anything particularly general without
linguistic faculties. This contention may find both empirical or philosophical
dispute, but the point is to make a first order approximation for the sake of
this presentation and perspective. The debate of mathematics and its relation to
linguistics generally, regardless of the stance one takes, should hopefully
benefit from the work presented in this thesis regardless of ones stance.

  * syntactic Completeness and semantic adequacy

The GF pipeline, that of bidirectional translation through an intermediary
abstract syntax representation, has, two fundamental criteria that must be
assessed for one to judge the success of the approach for the informal/formal
translation. The first, which we'll call *syntactic completeness*, asks the
following : given an utterance or natural language expression that a
mathematician might understand, does the GF grammar emit a well-formed syntactic
expression in the target logic or programming language? [Example?]

This problem is certain to be infeasible in many cases - a mathematician might
not be able to reconstruct the unstated syntactic details of a proof in an
discipline outside her expertise, it is at worthy pursuit to ask why it is so
difficult! Additionally, one does not know a priori that the generated
expression in the logic has its intended meaning, other than through some meta
device (like for instance, some meaning explanation outside verification
procedure).

Conversely, given a well formed syntactic expression in, for instance, Agda, one
can ask if the resulting English expression generated by GF is *semantically
adequate*. This notion of semantic adequacy is also delicate, as mathematicians
themselves may dispute, for instance, the proof of a given proposition or the
correct definition of some notion. However, if it is doubtful that there would
be many mathematicians who would not understand some standard theorem/proof pair
in a 7th edition introductory real analysis text, even if they dispute it's
presentation, clarity, pedagogy, or other pedantic details. So, whether one asks
that semantic adequacy means some kind of sociological consensus among those
with relevant expertise, or a more relaxed criterion that some expert herself
understands the argument (a possibly dubious perspective in science), semantic
adequacy should appease at least one and potentially more mathematicians.

We introduce these terms, syntactic completeness and semantic adequacy, to
highlight a perspective and insight that seems to underlie the biggest
differences between informal and formal mathematics. We claim that mathematics,
as done on a theorem prover, is a syntax oriented endeavor, whereas mathematics,
as practiced by mathematicians, prioritizes semantic understanding.

This perspective represents an observation, and not intended to take a side as
to whether the syntactic or semantic perspective on mathematics is better -
there is an dialectical phenomena between the two.

Let's highlight some advantages both provide, and try to distinguish more
precisely what a syntactic and semantic perspective may be.

When the Agda user builds her proof, she is outsourcing much of the bookkeeping
to the type-checker. This isn't purely a mechanical process though, she often
does have to think, how her definitions will interact with typing and term
judgments downstream, as well as whether they are even sensible to begin with
(i.e. does this have a proof). The syntactic side is expressly clear from the
readers perspective as well. If Agda proofs were semantically coherent, one
would only need to look at code, with perhaps a few occasional remarks about
various intentions and conclusions, to understand the mathematics being
expressed. Yet, papers are often written exclusively in Latex, where Agda proofs
have had to be reverse engineered, preserving only semantic details and
forsaking syntactic nuance, and oftentimes the code is kept in the appendix so
as to provide a complete syntactic blueprint. But the act of writing an Agda
proof and reading them are often orthogonal, as the term somehow shadows the
application of typing rules which enable its construction. In some sense, the
construction of the proof is entirely engaged with the types, whereas the human
witness of a term is either lost as to why it fulfills the typing judgment, or
they have to reconstruct the proof in their head (or perhaps, again, with Agda's
help).

Even in cases where Agda code is included in the paper, it is often the types
that emphasized (and read), and complex proof terms are seldom to be read on
their own terms. The natural language description and commentary is still
largely necessary to convey whatever results, regardless if the Agda code is
self-contained. And while literate Agda is some type of bridge, it is still the
commentary, which in some sense unfolds the code, which makes the Agda legible.

This is particularly pronounced in Coq, where proof terms are built using LTac,
which can be seen as some kind of imperative syntactic metaprogramming over the
core language, Gallina. The Tactics themselves are not typed, often feel very
adhoc, and carry very little semantic value (or even possibly muddy one's
understanding when reading proofs with unknown tactics). Indeed, since LTac
isn't itself typed, it often descends into the sorrows of so-called untyped
languages (which really are really unityped), and there are multiple arguements
that this should be changed. But from our perspective, the use of tactics is an
additional syntactic obfuscation of what a proof should look like from the
mathematicians perspective - and attempt to remedy this is. This isn't always
the case, however, as tactics like `ring` or `omega` often save the reader
overhead of parsing pendantic and uninforamtive details. And for certain proofs,
especially those involving many cases, the metaprogramming facilities actually
give one exclusive advantages not offered to the classical mathematician using
pen and paper. Nonetheless, the dependent type theorist's dream that all
mathematicians begin using theorem provers in their everyday work is largely
just a dream, and with relatively little mainstream adoption by mathematicians,
the future is all but clear.

Mathematicians may indeed like some of the facilities theorem provers provide,
but ultimately, they may not see that as the "essence" of what they are doing.
