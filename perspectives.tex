\section{Perspectives}

\begin{displayquote}

...when it comes to understanding the power of mathematical language to guide our
thought and help us reason well, formal mathematical languages like the ones
used by interactive proof assistants provide informative models of informal
mathematical language. The formal languages un- derlying foundational frameworks
such as set theory and type theory were designed to provide an account of the
correct rules of mathematical reasoning, and, as Gödel observed, they do a
remarkably good job. But correctness isn’t everything: we want our mathematical
languages to enable us to reason efficiently and effectively as well. To that
end, we need not just accounts as to what makes a mathematical argument correct,
but also accounts of the structural features of our theorizing that help us
manage mathematical complexity.\cite{avigad2015mathematics}

\end{displayquote}

The key development of this thesis it to explore the formal/informal distinction
in mathematical language via by means of rule-based, syntax oriented machine
translation.

Computational linguistics, particularly those in the tradition of type
theoretical semantics\cite{ranta1994type}, gives one a way of comparing natural
and programming languages. Although it is concerned with the semantics of
natural language in the logical tradition of Montague, which who synthesized
work in the traditions of Chomsky \cite{Chomsky57} and Frege \cite{frege79}, it
ended up inspiring the GF system, a side effect of which was to realize that
translation was possible with this of abstracted view of natural language
semantics. One can then carry this analogy to other areas of computer science,
and compare NL and PL phenomena. Indeed, one such description of GF is that it
is a compiler tool applied to domain specific machine translation.

* In PL theory, we also have a parallel series of abstraction levels, of which a
compiler writing course covers the basics. Lexemes -> Abstract Syntax Trees ->
Well Formed ASTs (i.e. trees which type checking) -> (possibly) intermediate
language Programs -> Target Language * In what I'll call formal linguistics, we
have some kind of hierarchy in terms of the following Phonemes -> Morhemes ->
Syntactic Units -> Sentences -> Dialogue

\begin{tikzcd}
Strings \ar[d,"Lexical\ Analysis"] \ar[dd,bend right=+90.0, swap,"Front\ End"]
&[5m]
\\ Lexemes \ar[d,"Parsing"] &[5em]
\\ ASTs \ar[d,"Type\ Checker"] &[5m]
\\ Typed\ ASTs
  \ar[dd, bend left, "Code\ Generator"] 
  \ar[dd, bend right, swap, "Interpreter"] &[5m]
\\ ...
\\ Target\ Language
\end{tikzcd}

\begin{tikzcd}
Strings \ar[d,"Lexical\ Analysis"] \ar[dd,bend right=+90.0, swap,"Front\ End"]
&[5m]
\\ Lexemes \ar[d,"Parsing"] &[5em]
\\ ASTs \ar[d,"Type\ Checker"] &[5m]
\\ Typed\ ASTs
  \ar[dd, bend left, "Code\ Generator"] 
  \ar[dd, bend right, swap, "Interpreter"] &[5m]
\\ ...
\\ Target\ Language
\end{tikzcd}
\hspace{1cm}
\begin{tikzcd}
Phonemes \arrow[d, "Morhphophonological\\ Anaylsis" description]                                                                      \\
Morphemes \arrow[d, "Parse"]                                                                                                          \\
\{\ Syntactic\ Representation\ \} \arrow[d, "Montague"', bend right=49] \arrow[d, "Ranta", bend left=49] \arrow[d, "..." description] \\
{\{\ STLC,\ ...\ ,\ DTT\ \}} \arrow[d, "?" description]                                                                               \\
{\{\ Nearal Encoding\ ,\ ...\ Internal\ Language\ \}} \arrow[d, "?" description]                                                      \\
Phonemes                                                                                                                             
\end{tikzcd}

% \begin{tikzcd}
%      & Phonemes \arrow[d, "Morhphophonological\\ Anaylsis" description]                                            &     \\
%      & Morphemes \arrow[d, "Parse"]                                                                                &     \\
%      & Phrase\ Structure\ Tree \arrow[ld, "Montague" description] \arrow[rd, "Ranta" description] \arrow[d, "..."] &     \\
% STLC & ...(semantic model)... \arrow[d, "?" description]                                                           & DTT \\
%      & {\{\ Nearal Encoding\ ,\ ...\ Internal\ Language\ \}} \arrow[d, "?" description]                            &     \\
%      & Phonemes                                                                                                    &    
% \end{tikzcd}
% \begin{tikzcd}
% & Phonemes \arrow[d, "Morhphophonological\\ Anaylsis" description] & \\
% & Morphemes \arrow[d, "Parse"] & \\
% & Phrase\ Structure\ Tree \arrow[ld, "Montague" description] \arrow[rd, "Ranta"
% description] \arrow[d, "..."] & \\
% STLC \arrow[ruuu, "Internal Language", bend left] & ... \arrow[d, "?"
% description] & DTT \arrow[luuu, bend right] \\
% {\{\ Neural\ Encoding,} & ... \arrow[d, "?"] & {,\ Internal\ Language\ \}} \\
% & Phonemes &
% \end{tikzcd}

% \begin{tikzcd}
% & Phonemes \arrow[d, "Morhphophonological\\ Anaylsis" description] & \\
% & Morphemes \arrow[d, "Parse"] & \\
% & Phrase\ Structure\ Tree \arrow[ld, "Montague" description] \arrow[rd, "Ranta"
% description] \arrow[d, "..."] & \\
% STLC \arrow[ruuu, "Internal Language", bend left] & ...(semantic model)... \arrow[d, "?" description]                                                           & DTT \arrow[luuu, bend right] \\
% & {\{\ Nearal Encoding\ ,\ ...\ Internal\ Language\ \}} \arrow[d, "?"] & \\
% & Phonemes &
% \end{tikzcd}

% \begin{tikzcd}
%      & A \arrow[d, "f"]                              &     \\
%      & Phrase\ Structure\ Tree \arrow[ld] \arrow[rd] &     \\
% STLC &                                               & DTT
% \end{tikzcd}

% \begin{tikzcd}
%      & A \arrow[d, "f"]                                &     \\
%      & Phrase\\ Structure\\ Tree \arrow[ld] \arrow[rd] &     \\
% STLC &                                                 & DTT
% \end{tikzcd}

% \begin{tikzcd}[column sep=small]
% & A \arrow[d] & \\
% & A' \arrow[dl] \arrow[dr] & \\
% B \ar{uur} &  & C \ar{uul}
% \end{tikzcd}

% \begin{tikzcd}
% & A \arrow[d, "f" description] & & \\
% & A' \arrow[ld] \arrow[rd] & & \\
% B \arrow[ruu, bend left] & & C \arrow[luu, bend right] &
% \end{tikzcd}

The first diagram exhibits 

% \begin{tikzcd}[column sep=small]
%   B \arrow[dr] & & C \arrow[dl] \\
%   & A \arrow[d] \\
%   & A' \ar[uul] \ar[uur] 
% \end{tikzcd}


% \begin{tikzcd}[column sep=small]
%   & A \arrow[dl] \arrow[dr] & \\
%   B \arrow{rr} & & C
% \end{tikzcd}


This diagram is perhaps the most important side-by-side comparison one should
observe and retain while reading this. Importantly, we should observe that the
lefthand diagram represents synthetic processes, those which we decide and
describe formally, whereas those on the right represent analytic observations,
and therefore are subject to different, in some ways orthogonal, constraints.



Let's zoom in a little and observe the so-called front-end part of the compiler. 

\begin{tikzcd}
Strings \ar[r,"Lexical Analysis"] \ar[rr,bend right,"GF Parser"'] &[10em] Lexemes
\ar[r,"Parsing"] &[10em] ASTs \ar[ll,bend right, "GF Linearization"] 
\end{tikzcd}

This also has some (external) mapping at the neurological level, where one
somehow can say the internal language, mechanism of externalization (generally
speech), but we won't be concerned with that here. The point is to recognize
their are stark differences, and that classifying both programming languages and
natural languages as languages is best read as an incomplete (and even sometimes
contradictory) metaphor, due to perceived similarities (of which their are
ample).

Nonetheless, the point of this thesis is to take a crack at that exact question
: how can one compare programming and natural languages, in the sense that a
natural language, when restricted to a small enough (and presumably
well-behaved) domain, behaves as a programming language. And simultaneously, we
probe the topic of Natural Language Generation (NLG), in the sense that given a
logic or type system with some theory inside (say arithmetic over the naturals),
how do we not just find a natural language representation which interprets our
expressions, but also does so in a way that is linguistically coherent in a
sense that a competent speaker can read such an expression and make sense of it.

The specific linguistic domain we focus on, that of mathematics, is a particular
sweet spot in the intersection of many intersecting interests. It should be
noted that this problem, that of translating between formal (in a PL or logic)
and informal (in linguistic sense) mathematics as stated, is both vague and
difficult. It is difficult in both the computer science sense, that it may be
either of infeasible complexity or even perhaps undecidable. But it is also
difficult in the philosophical sense, that it a question which one may come up
with a priori arguements against its either effectiveness or meaningfulness.
Because, like all collective human endeavors, mathematics is a historical
construction - that is, its conventions, notations, understanding,
methodologies, and means of publication and distribution have all been in a
constant flux. While in some sense the mathematics today can be seen today as a
much refined version of whatever the Greeks or Egyptians were doing, there is no
consensus on what mathematics is, how it is to be done, and most relevant for
this treatise, how it is to be expressed.

We present a sketch of the difference of this so-called formal/informal
distinction. Mathematics, that is mathematical constructions like numbers and
geometrical understandings, arose out of ad-hoc needs as humans cultures grew
and evolved over the millennia. Indeed, just like many of the most interesting
human developments of which there is a sparsely documented record until
relatively recently, it is likely to remain a mystery what the long historical
arc of mathematics could have looked like in the context of human evolution. And
while mathematical notions precede mathematical constructions (the spherical
planet precedes the human use of a ruler compass construction to generate a
circle), we should take it as a starting point that mathematics arises naturally
out of our linguistic capacity. This may very well not be the case, or at least,
not the case in all circumstances. But it is impossible for me to imagine a
mathematical construction elaborating anything particularly general without
linguistic faculties. This contention may find both empirical or philosophical
dispute, but the point is to make a first order approximation for the sake of
this presentation and perspective. The debate of mathematics and its relation to
linguistics generally, regardless of the stance one takes, should hopefully
benefit from the work presented in this thesis regardless of ones stance.

  * syntactic Completeness and semantic adequacy

The GF pipeline, that of bidirectional translation through an intermediary
abstract syntax representation, has, two fundamental criteria that must be
assessed for one to judge the success of the approach for the informal/formal
translation. The first, which we'll call *syntactic completeness*, asks the
following : given an utterance or natural language expression that a
mathematician might understand, does the GF grammar emit a well-formed syntactic
expression in the target logic or programming language? [Example?]

This problem is certain to be infeasible in many cases - a mathematician might
not be able to reconstruct the unstated syntactic details of a proof in an
discipline outside her expertise, it is at worthy pursuit to ask why it is so
difficult! Additionally, one does not know a priori that the generated
expression in the logic has its intended meaning, other than through some meta
device (like for instance, some meaning explanation outside verification
procedure).

Conversely, given a well formed syntactic expression in, for instance, Agda, one
can ask if the resulting English expression generated by GF is *semantically
adequate*. This notion of semantic adequacy is also delicate, as mathematicians
themselves may dispute, for instance, the proof of a given proposition or the
correct definition of some notion. However, if it is doubtful that there would
be many mathematicians who would not understand some standard theorem/proof pair
in a 7th edition introductory real analysis text, even if they dispute it's
presentation, clarity, pedagogy, or other pedantic details. So, whether one asks
that semantic adequacy means some kind of sociological consensus among those
with relevant expertise, or a more relaxed criterion that some expert herself
understands the argument (a possibly dubious perspective in science), semantic
adequacy should appease at least one and potentially more mathematicians.

We introduce these terms, syntactic completeness and semantic adequacy, to
highlight a perspective and insight that seems to underlie the biggest
differences between informal and formal mathematics. We claim that mathematics,
as done on a theorem prover, is a syntax oriented endeavor, whereas mathematics,
as practiced by mathematicians, prioritizes semantic understanding.

This perspective represents an observation, and not intended to take a side as
to whether the syntactic or semantic perspective on mathematics is better -
there is an dialectical phenomena between the two.

Let's highlight some advantages both provide, and try to distinguish more
precisely what a syntactic and semantic perspective may be.

When the Agda user builds her proof, she is outsourcing much of the bookkeeping
to the type-checker. This isn't purely a mechanical process though, she often
does have to think, how her definitions will interact with typing and term
judgments downstream, as well as whether they are even sensible to begin with
(i.e. does this have a proof). The syntactic side is expressly clear from the
readers perspective as well. If Agda proofs were semantically coherent, one
would only need to look at code, with perhaps a few occasional remarks about
various intentions and conclusions, to understand the mathematics being
expressed. Yet, papers are often written exclusively in Latex, where Agda proofs
have had to be reverse engineered, preserving only semantic details and
forsaking syntactic nuance, and oftentimes the code is kept in the appendix so
as to provide a complete syntactic blueprint. But the act of writing an Agda
proof and reading them are often orthogonal, as the term somehow shadows the
application of typing rules which enable its construction. In some sense, the
construction of the proof is entirely engaged with the types, whereas the human
witness of a term is either lost as to why it fulfills the typing judgment, or
they have to reconstruct the proof in their head (or perhaps, again, with Agda's
help).

Even in cases where Agda code is included in the paper, it is often the types
that emphasized (and read), and complex proof terms are seldom to be read on
their own terms. The natural language description and commentary is still
largely necessary to convey whatever results, regardless if the Agda code is
self-contained. And while literate Agda is some type of bridge, it is still the
commentary, which in some sense unfolds the code, which makes the Agda legible.

This is particularly pronounced in Coq, where proof terms are built using LTac,
which can be seen as some kind of imperative syntactic metaprogramming over the
core language, Gallina. The Tactics themselves are not typed, often feel very
adhoc, and carry very little semantic value (or even possibly muddy one's
understanding when reading proofs with unknown tactics). Indeed, since LTac
isn't itself typed, it often descends into the sorrows of so-called untyped
languages (which really are really unityped), and there are multiple arguements
that this should be changed. But from our perspective, the use of tactics is an
additional syntactic obfuscation of what a proof should look like from the
mathematicians perspective - and attempt to remedy this is. This isn't always
the case, however, as tactics like `ring` or `omega` often save the reader
overhead of parsing pendantic and uninforamtive details. And for certain proofs,
especially those involving many cases, the metaprogramming facilities actually
give one exclusive advantages not offered to the classical mathematician using
pen and paper. Nonetheless, the dependent type theorist's dream that all
mathematicians begin using theorem provers in their everyday work is largely
just a dream, and with relatively little mainstream adoption by mathematicians,
the future is all but clear.

Mathematicians may indeed like some of the facilities theorem provers provide,
but ultimately, they may not see that as the "essence" of what they are doing.

----------------------------------------------------------------

What is a proof? 

Let
Human 1 := h1
Human 2 := h2
. If h1 claims to have a proof p1, and elaborates it to h2 who claims she can
either verify p1 or 
reproduce and re-articulate it via p1', such that h1 and h2 agree that p1 and
p1' are equivalent, then we have discovered some mathematics.agree Consensus of h1


  Formalization = formal model construction of an informal piece of text
  model could be different for different formalizations

  - it may be formalized differently by two different people in many different ways
  - it may have to be modified, to correct of an error, or modify a smaller piece?
  - it may not type check, and only be presumed hypothetically to be 'a correct formalization'


  
What is a proof? It is a series of lemmas, constructors, and destructors

What is informalization? 

It is a process of taking a formal syntax, and turning it into a natural
language utterance -  It is a clarification of the meaning of a piece of
code. suppressing certain details and reiterating, sometimes
redundantly, other details.

A metaphor of the architect and the engineer.  A building, like all human
endeavors, is created via resources and labor of many people. The role of the archtitect
is to envision the gilded facade, the exterior layer directly perceived by
others, giving a building its character, its purpose and function hidden in
either specific articulations like the
smokestack,  its 

Dialectal layers of mathematics.  From the formal to the expository 

Chicken or the egg?

In order to construct a model of mathematics, we must a priori have a
mathematical out of which to construct it.  Meaning Explanations?

What is a foundation of mathematics from a mathematical perspective vs from a
philosophical perspective

