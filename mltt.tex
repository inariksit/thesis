% from blog post
\subsection{Martin-Löf Type Theory}
\subsubsection{Judgments}

\begin{displayquote}

With Kant, something important happened, namely, that the term judgement, Ger.
Urteil, came to be used instead of proposition \cite{mlMeanings}.

\end{displayquote}

A central contribution of Per Martin-Löf in the development of type theory was
the recognition of the centrality of judgments in logic. Many mathematicians
aren't familiar with the spectrum of judgments available, and merely believe
they are concerned with \emph{the} notion of truth, namely \emph{the truth} of a
mathematical proposition or theorem. There are many judgments one can make which
most mathematicians aren't aware of or at least never mention. Examples of both familiar
and unfamiliar judgments include,

\begin{itemize}

\item $A$ is true
\item $A$ is a proposition
\item $A$ is possible
\item $A$ is necessarily true
\item $A$ is true at time $t$

\end{itemize}

These judgments are understood not in the object language in which we state our
propositions, possibilities, or probabilities, but as assertions in the
metalanguage which require evidence for us to know and believe them. Most
mathematicians may reach for their wallets if I come in and give a talk saying
it is possible that the Riemann Hypothesis is true, partially because they
already know that, and partially because it doesn't seem particularly
interesting to say that something is possible, in the same way that a physicist
may flinch if you say alchemy is possible. Most mathematicians, however, would
agree that $P = NP$ is a proposition, and it is also possible, but isn't true.

For the logician these judgments may well be interesting because their may be
logics in which the discussion of possibility or necessity is even more
interesting than the discussion of truth. And for the type theorist interested
in designing and building programming languages over many various logics, these
judgments become a prime focus. The role of the type-checker in a programming
language is to present evidence for, or decide the validity of the judgments.
The four main judgments of type theory are given in natural language on the left
and symbolically on the right :

\begin{multicols}{2}
\begin{itemize}
\item $T$ is a type
\item $T$ and $T'$ are equal types
\item $t$ is a term of type $T$
\item $t$ and $t'$ are equal terms of type $T$
\item $\vdash T \; {\rm type}$
\item $\vdash T = T'$
\item $\vdash t:T$
\item $\vdash t = t':T$
\end{itemize}
\end{multicols}

Frege's turnstile, $\vdash$, denotes a judgment. These judgments become much more interesting when we add the ability for them to
be interpreted in a some context with judgment hypotheses. Given a series of
judgments $J_1,...,J_n$, denoted $\Gamma$, where $J_i$ can depend on previously
listed $J's$, we can make judgment $J$ under the hypotheses, e.g. $J_1,...,J_n
\vdash J$. Often these hypotheses $J_i$, alternatively called \emph{antecedents},
denote variables which may occur freely in the *consequent* judgment $J$. For
instance, the antecedent, $x : \mathbb{R}$ occurs freely in the syntactic
expression $\sin x$, a which is given meaning in the judgment $\vdash \sin x { :
} \mathbb{R}$. We write our hypothetical judgement as follows :

$$x : \mathbb{R} \vdash \sin x : \mathbb{R}$$



\subsubsection{Rules}

Martin-Löf systematically used the four fundamental judgments in the proof
theoretic style of Pragwitz and Gentzen. To this end, the intuitionistic formulation of the
logical connectives just gives rules which admit an immediate computational
interpretation. The main types of rules are type formation, introduction,
elimination, and computation rules. The introduction rules for a type admit an
induction principle derivable from that type's signature. Additionally, the
$\beta$ and $\eta$ computation rules are derivable via the composition of
introduction and elimination rules, which, if correctly formulated, should
satisfy a relation known as harmony.

The fundamental notion of the lambda calculus, the function, is 
abstracted over a variable and returns a term of some type when applied to an
argument which is subsequently reduced via the computational rules.
Dependent Type Theory (DTT) generalizes this to allow the return type be
parameterized by the variable being abstracted over. The dependent function
forms the basis of the LF which underlies Agda and GF. Here is the formation
rule : 

\[
  \begin{prooftree}
    \hypo{̌\Gamma  \vdash A \; {\rm type}}
    \hypo{̌\Gamma, x : A \vdash B \; {\rm type}}
    \infer2[]{\Gamma \vdash \Pi x {:} A. B}
  \end{prooftree}
\]

One reason why hypothetical judgments are so interesting is we can devise rules
which allow us to translate from the metalanguage to the object language using
lambda expressions. These play the role of a function in mathematics and
implication in logic. More generally, this is a dependent type, representing the
$\forall$ quantifier. Assuming from now on $\Gamma \vdash A \; {\rm type}$ and
$\Gamma, x : A \vdash B \; {\rm type}$, we present here the introduction rule for
the most fundamental type in Agda, denoted \term{(x : A) -> B}.

\[
  \begin{prooftree}
    \hypo{̌\Gamma, x {:} A \vdash B \; {\rm type}}
    \infer2[]{\Gamma \vdash \lambda x. b {:} \Pi x {:} A. B}
  \end{prooftree}
\]

Observe that the hypothetical judgment with $x : A$ in the hypothesis has been
reduced to the same hypothesis set below the line, with the lambda term and Pi
type now accounting for the variable.

\[
  \begin{prooftree}
    \hypo{\Gamma \vdash f {:} \Pi x {:} A. B}
    \hypo{\Gamma \vdash a {:} A}
    \infer2[]{\Gamma \vdash f\, a {:} B[x := a]}
  \end{prooftree}
\]

We briefly give the elimination rule for
Pi, application, as well as the classic $\beta$ and $\eta$ computational equality
judgments (which are actually rules, but it is standard to forego the premises): 
$$\Gamma \vdash (\lambda x.b)\, a \equiv b[x := a] {:} B[x := a]$$
$$\Gamma \vdash (\lambda x.f)\, x \equiv f {:} \Pi x{:}A. B}$$
Using this rule, we now see a typical judgment without hypothesis in a real
analysis, $\vdash \lambda x. \sin x : \mathbb{R} \rightarrow \mathbb{R}$.  This is normally
expressed as follows (knowing full well that $\sin$ actually has to be
approximated when saying what the computable function in the codomain is): 
\begin{align*}
  \sin {:} \mathbb{R} &\rightarrow \mathbb{R}\\ x &\mapsto \sin ( x )
\end{align*}
Evaluating this function on 0, we see
\begin{align*}
(\lambda x. \sin x)\, 0 &\equiv \sin 0   \\ &\equiv 0
\end{align*}

While most mathematicians take this for granted, we hope this gives some insight
into how computer scientists present functions. We recommend reading
Martin-Löf's original papers \cite{ml1984} \cite{ml79} to see all the rules
elaborated in full detail, as well as his philosophical papers
\cite{mlMeanings} \cite{mlTruth} to understand type theory as it was conceived
both practically and philosophically.

\subsection{Propositions, Sets, and Types}

While the rules of type theory have been well-articulated elsewhere, we provide
briefly compare the syntax of mathematical constructions in FOL, one possible
natural language use \cite{rantaLog}, and MLTT. From this vantage, these look
like simple symbolic manipulations, and in some sense, one doesn't need a the
expressive power of system like GF to parse these to the same form.

Additionally, it is worth comparing the type theoretic and natural language
syntax with set theory, as is done in \autoref{fig:P1} and \autoref{fig:P2}. Now
we bear witness to some deeper cracks than were visible above. We note that the
type theoretic syntax is \emph{the same} in both tables, whereas the set
theoretic and logical syntax shares no overlap. This is because set theory and
first order logic are distinct domains classically, whereas in MLTT,
there is no distinguishing mathematical types from logical types - everything is
a type.

\begin{figure}[H]
\centering
\begin{tabular}{|c|c|c|c|} \hline
  FOL & MLTT & NL FOL & NL MLTT \\ \hline
  $\forall\ x\ P(x)$ & $\Pi x : \tau.\ P(x)$     & $for\ all\ x,\ p$  & $the\  product\  over\  x\  in\ p$ \\ 
  $\exists\ x\ P(x)$ & $\Sigma x : \tau.\ P(x)$  & $there\ exists\ an\ x\ such\ that\ p$ & $there\ exists\ an\ x\ in\ \tau such\ that\ p$ \\ 
  $p\ \supset\ q$    & $p\ \to\ q$               & $if\ p\ then\ q$   & $p\  to\  q$ \\ 
  $p\ \wedge\ q$     & $p\ \times\ q$            & $p\ and\ q$        & $the\  product\  of\  p\  and\  q$ \\ 
  $p\ \lor\ q$       & $p\ +\ q$                 & $p\ or\ q$         & $the\  coproduct\  of\  p\  and\  q$ \\ 
  $\neg\ p$          & $\neg\ p$                 & $it\ is\ not\ the\ case\ that\ p$ & $not\ p$ \\ 
  $\top$             & $\top$                    & $true$             & $top$ \\ 
  $\bot$             & $\bot$                    & $false$            & $bottom$ \\ 
  $p\ =\ q$          & $p\ \equiv\ q$            & $p\ equals\ q$     & $definitionally\  equal$ \\ \hline
\end{tabular}
\caption{FOL vs MLTT} \label{fig:P1}
\end{figure}

We show the Type and set comparisons in \autoref{fig:P2}. The basic types are
sometimes simpler to work with because they are expressive enough to capture
logical and set theoretic notions, but this also comes at a cost. The union of
two sets simply gives a predicate over the members of the sets, whereas union
and intersection types are often not considered ``core" to type theory, with
multiple possible ways of interpreting how to treat this set-theoretic concept.
The behavior of subtypes and subsets, while related in some ways, also
represents a semantic departure from sets and types. For example, while there
can be a greatest type in some sub-typing schema, there is no notion of a top
set. This is why we use the type theoretic NL syntax when there are question
marks in the set theory column.


\begin{figure}[H]
\centering
\begin{tabular}{|c|c|c|c|} \hline
 Set Theory & MLTT & NL Set Theory & NL MLTT \\ \hline
 $S$          & $\tau$                 & $the\ set\ S$                     & $the\ type\ \tau$ \\ 
 $\mathbb{N}$ & $Nat$                  & $the\ set\ of\ natural\ numbers$  & $the\ type\ nat$ \\
 $S \times T$ & $S \times T$           & $the\ product\ of\ S\ and\ T$     & $the\  product\  of\  S\  and\  T$ \\
 $S \to T$    & $S \to T$              & $the\ function\ \from\ S\ to\ T$  & $p\  to\  q$ \\
 $\{x|P(x)\}$ & $\Sigma x : \tau.\ P(x)$ & $the\ set\ of\ x\ such\ that\ P$  & $there\ exists\ an\ x\ in\ \tau such\ that\ p$ \\
  $\emptyset$  & $\bot$                 & $the\ empty\ set$                 & $bottom$ \\
 $?$          & $\top$                 & $?$                             & $top$ \\
 $S \cup T$   & $?$                    & $the\ union\ of\ S\ and\ T$       & $?$ \\
 $S \subset T$ & $S <: T$              & $S\ is\ a\ subset\ of\ T$          & $S\ is\ a\ subtype\ of\ T$ \\
 $?$          & $U_1$                  & $?$ & $the\ second\ Universe$        \\ \hline 
\end{tabular}
\caption{Sets vs MLTT} \label{fig:P2}
\end{figure}


We also note that pragmatically, type theorists often interchange the logical,
set theoretic, and type theoretic lexicons when describing types. Because the
types were developed to overcome shortcomings of set theory and classical logic,
the lexicons of all three ended up being blended, and in some sense, the type
theorist can substitute certain words that a classical mathematician
wouldn't.  Whereas $p\ implies\ q$ and $function\ from\ X\ to\ Y$ are not to
be mixed, the type theorist may in some sense default to either.
Nonetheless, pragmatically speaking, one would never catch a type theorist
saying $Nat implies Nat$ when expressing $Nat\ \to\ Nat$.


Terms become even messier, and this can be seen in just a small sample shown in
\autoref{fig:P3}. In simple type theory, one distinguishes between types and
terms at the syntactic level - this disappears in DTT. As will be seen later,
the mixing of terms and types gives MLTT an incredible expressive power, but
undoubtedly makes certain things very difficult as well. In set theory,
everything is a set, so there is no distinguishing between elements of sets and
sets even though practically they function very differently. Mathematicians only
use sets because of their flexibility in so many ways, not because the axioms of
set theory make a compelling case for sets being this kind of atomic form that
makes up the mathematical universe. Category theorists have discovered vast
generalizations of sets (where elements are arrows) which allow one to have the
flexibility in a more structured and nuanced way, and the comparison with
categories and types is much tighter than with sets. Regardless, mathematicians
day to day work may not need all this general infrastructure.

In FOL, terms don't exist at all, and the proof rules themselves contain the
necessary information to encode the proofs or constructions. The type theoretic
terms somehow compress and encode the proof trees, of which, and in the case of
ITPs nodes are displayed during the interactive type-checking phase.

\begin{figure}[H]
\centering
\begin{tabular}{|c|c|c|c|c|} \hline
 Set Theory & MLTT & NL Set Theory & NL MLTT & Logic \\ \hline
 $f(x) := p$ & $\lambda x. p$ & $f\ of\ x\ is\ p$ & $lambda\ x,\ p$ & $\supset-elim$ \\
 $f(p)$ & $f p$ & $f of p$ & $the\ application\ of\ f\ to\ p$ & $modus\ ponens$ \\
 $(x,y)$          & $(x,y)$ & $the\ pair\ of\ x\ and\ y$ & $the\ pair\ of\ x\ and\ y$ &  $\wedge-i$ \\
 $\pi_{1,2}\ x$      & $\pi_{1,2}\ x$ & $the\ first\ projection\ of\ x$ & $the\ first\ projection\ of\ x$ & $\wedge-e$ \\ \hline
\end{tabular}
\caption{Term syntax in Sets, Logic, and MLTT} \label{fig:P3}
\end{figure}

We don't do all the constructors for type theory here for space, but note some
interesting features:

\begin{itemize}
\item The disjoint union in set theory is actually defined using 
pairs - and therefore it doesn't have elimination forms other than those
for the product. The disjoint union is also not nearly as ubiquitous, though.
\item $\lambda$ is a constructor for both the dependent and
non-dependent function, so its use in either case will be type-checked by Agda,
whereas it's natural language counterpart in real mathematics will have
syntactic distinction.
\item The projections for a $\Sigma$ type behaves differently from the
elimination principle for $\exists$, and this leads to
incongruities in the natural language presentation.
\end{itemize}

Finally, we should note that there are many linguistic presentations
mathematicians use for logical reasoning, i.e. the use of introduction and
elimination rules. They certainly seem to use linguistic forms more when dealing
with proofs, and symbolic notation for Sets, so the investigation of how these
translate into type theory is a source of future work. Whereas propositions make
explicit all the relevant detail, and can be read by non-experts, proofs are
incredibly diverse and will be incomprehensible to those without expertise. 

A detailed analysis of this should be done if and when a proper translation
corpus is built to account for some of the ways mathematicians articulate these
rules, as well as when and how mathematicians discuss sets,
symbolically and otherwise. To create translation with ``real" natural language 
is likely not to be very effective or interesting without a lot of evidence about
how mathematicians speak and write.